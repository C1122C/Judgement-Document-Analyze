# coding: utf-8
from __future__ import print_function
import tensorflow as tf
import numpy as np
import time
import os


def pick_top_n(preds, vocab_size, top_n=5):
    p = np.squeeze(preds)
    # 将除了top_n个预测值的位置都置为0
    p[np.argsort(p)[:-top_n]] = 0
    # 归一化概率
    p = p / np.sum(p)
    # 随机选取一个字符
    c = np.random.choice(vocab_size, 1, p=p)[0]
    return c


class CharRNN:
    def __init__(self):
        self.lstm_size = 128
        self.num_layers = 2
        self.LEARNING_RATE = 0.001  # 学习率
        self.EMBEDDING_DIM = 128  # 词向量维度
        self.FACT_LEN = 30  # 事实长度
        self.LAW_LEN = 30  # 法条长度
        self.KS_LEN = 3  # 先验知识长度
        self.NUM_CLASS = 2  # 类别数
        self.LEARNING_RATE = 0.001  # 学习率
        self.batch_size = 128  # 批处理参数，每次训练跑的数据集样本数
        self.dropout_keep_prob = 0.5  # 以0.5的概率去除部分连接防止过拟合

        tf.reset_default_graph()
        self.build_inputs()
        self.build_lstm()
        self.build_loss()
        self.build_optimizer()
        self.saver = tf.train.Saver()

    def build_inputs(self):
        with tf.name_scope('inputs'):
            self.input_x1 = tf.placeholder(tf.float32, [self.batch_size, self.FACT_LEN, self.EMBEDDING_DIM],
                                           name='input_x1')  # 输入1：事实[ ,30,128]
            self.input_x2 = tf.placeholder(tf.float32, [self.batch_size, self.LAW_LEN, self.EMBEDDING_DIM],
                                           name='input_x2')  # 输入2：法条[ ,30,128]
            self.input_y = tf.placeholder(tf.int32, [self.batch_size, self.NUM_CLASS],
                                          name='input_y')  # 输入4：分类数[ ,2]


    def build_lstm(self):
        # 创建单个cell并堆叠多层
        def get_a_cell(lstm_size, keep_prob):
            lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)
            drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)
            return drop

        with tf.name_scope('lstm'):
            cell1 = tf.nn.rnn_cell.MultiRNNCell(
                [get_a_cell(self.lstm_size, self.dropout_keep_prob) for _ in range(self.num_layers)]
            )
            self.initial_state= cell1.zero_state(30, tf.float32)

            # 通过dynamic_rnn对cell展开时间维度
            lstm_output1, final_state1 = tf.nn.dynamic_rnn(cell1, self.input_x1, initial_state=self.initial_state)

            cell2 = tf.nn.rnn_cell.MultiRNNCell(
                [get_a_cell(self.lstm_size, self.dropout_keep_prob) for _ in range(self.num_layers)]
            )
            lstm_output2, final_state2 = tf.nn.dynamic_rnn(cell2, self.input_x2, initial_state=final_state1)
            # 通过lstm_outputs得到概率
            seq_output = tf.concat(lstm_output2, 1)
            x = tf.reshape(seq_output, [-1, self.lstm_size])

            with tf.variable_scope('softmax'):
                softmax_w = tf.Variable(tf.truncated_normal([self.lstm_size, self.NUM_CLASS], stddev=0.1))
                softmax_b = tf.Variable(tf.zeros(self.NUM_CLASS))

            self.logits = tf.matmul(x, softmax_w) + softmax_b
            self.proba_prediction = tf.nn.softmax(self.logits, name='predictions')

    def build_loss(self):
        with tf.name_scope('loss'):
            y_one_hot = tf.one_hot(self.input_y, self.NUM_CLASS)
            y_reshaped = tf.reshape(y_one_hot, self.logits.get_shape())
            loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=y_reshaped)
            self.loss = tf.reduce_mean(loss)

    def build_optimizer(self):
        # 使用clipping gradients
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)
        train_op = tf.train.AdamOptimizer(self.LEARNING_RATE)
        self.optimizer = train_op.apply_gradients(zip(grads, tvars))

    def train(self):
        self.session = tf.Session()
        with self.session as sess:
            sess.run(tf.global_variables_initializer())
            # Train network
            step = 0
            new_state = sess.run(self.initial_state1)
            for x, y in batch_generator:
                step += 1
                start = time.time()
                feed = {self.inputs: x,
                        self.targets: y,
                        self.keep_prob: self.train_keep_prob,
                        self.initial_state: new_state}
                batch_loss, new_state, _ = sess.run([self.loss,
                                                     self.final_state,
                                                     self.optimizer],
                                                    feed_dict=feed)

                end = time.time()
                # control the print lines
                if step % log_every_n == 0:
                    print('step: {}/{}... '.format(step, max_steps),
                          'loss: {:.4f}... '.format(batch_loss),
                          '{:.4f} sec/batch'.format((end - start)))
                if (step % save_every_n == 0):
                    self.saver.save(sess, os.path.join(save_path, 'model'), global_step=step)
                if step >= max_steps:
                    break
            self.saver.save(sess, os.path.join(save_path, 'model'), global_step=step)

    def sample(self, n_samples, prime, vocab_size):
        samples = [c for c in prime]
        sess = self.session
        new_state = sess.run(self.initial_state)
        preds = np.ones((vocab_size, ))  # for prime=[]
        for c in prime:
            x = np.zeros((1, 1))
            # 输入单个字符
            x[0, 0] = c
            feed = {self.inputs: x,
                    self.keep_prob: 1.,
                    self.initial_state: new_state}
            preds, new_state = sess.run([self.proba_prediction, self.final_state],
                                        feed_dict=feed)

        c = pick_top_n(preds, vocab_size)
        # 添加字符到samples中
        samples.append(c)

        # 不断生成字符，直到达到指定数目
        for i in range(n_samples):
            x = np.zeros((1, 1))
            x[0, 0] = c
            feed = {self.inputs: x,
                    self.keep_prob: 1.,
                    self.initial_state: new_state}
            preds, new_state = sess.run([self.proba_prediction, self.final_state],
                                        feed_dict=feed)

            c = pick_top_n(preds, vocab_size)
            samples.append(c)

        return np.array(samples)

    def load(self, checkpoint):
        self.session = tf.Session()
        self.saver.restore(self.session, checkpoint)
        print('Restored from: {}'.format(checkpoint))
